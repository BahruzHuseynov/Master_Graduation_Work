{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-fBU_wRxqG6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('Alibaba-NLP/gte-base-en-v1.5', trust_remote_code=True)"
      ],
      "metadata": {
        "id": "_uFaiL0Sxsr5"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Trainable parameters: {trainable_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qnx-HWfMxzk5",
        "outputId": "3249c543-4520-42e7-f3f8-1e706c23a95b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable parameters: 136776192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model[0].max_seq_length = 512\n",
        "model[0].do_lower_case = True"
      ],
      "metadata": {
        "id": "r3elaM5Dxzz6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/rishabhmisra/News-Headlines-Dataset-For-Sarcasm-Detection.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVOxCazgx0iD",
        "outputId": "6080c4f6-1704-42b5-f49b-30ba6d0e3899"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'News-Headlines-Dataset-For-Sarcasm-Detection'...\n",
            "remote: Enumerating objects: 75, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 75 (delta 6), reused 0 (delta 0), pack-reused 62 (from 1)\u001b[K\n",
            "Receiving objects: 100% (75/75), 3.65 MiB | 28.76 MiB/s, done.\n",
            "Resolving deltas: 100% (35/35), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "\n",
        "file_path = 'News-Headlines-Dataset-For-Sarcasm-Detection/Sarcasm_Headlines_Dataset.json'\n",
        "with open(file_path, 'r') as file:\n",
        "    data = [json.loads(line) for line in file]\n",
        "\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "C5HcntvJx11_"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data, temp_data = train_test_split(df, test_size=0.2,  stratify=df['is_sarcastic'], random_state=42)\n",
        "val_data, test_data = train_test_split(temp_data, test_size=0.5, stratify=temp_data['is_sarcastic'], random_state=42)"
      ],
      "metadata": {
        "id": "-68oEyRlx3Ud"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"CUDA Available:\", torch.cuda.is_available())\n",
        "print(\"Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvYhieGsx4F0",
        "outputId": "aacbf1be-6e4d-4b2e-925d-af70f239d63c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Available: True\n",
            "Device Name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from transformers import AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer, InputExample\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from peft import PeftModel, PrefixTuningConfig\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "zTilCZNOx5VA"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(df, text_column, label_column):\n",
        "    lst = []\n",
        "    for _, row in df.iterrows():\n",
        "        lst.append(InputExample(texts=[row[text_column]], label=row[label_column]))\n",
        "    return lst\n",
        "\n",
        "train_examples = prepare_data(train_data, text_column=\"headline\", label_column=\"is_sarcastic\")\n",
        "validation_examples = prepare_data(val_data, text_column=\"headline\", label_column=\"is_sarcastic\")"
      ],
      "metadata": {
        "id": "NE1epeEmx6Rv"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    texts = [example.texts[0] for example in batch]\n",
        "    labels = [example.label for example in batch]\n",
        "\n",
        "    tokenized = model.tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
        "    labels = torch.tensor(labels, dtype=torch.float)\n",
        "\n",
        "    return tokenized, labels\n",
        "\n",
        "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16, collate_fn=collate_fn)\n",
        "validation_dataloader = DataLoader(validation_examples, shuffle=False, batch_size=16, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "BuDeI7Iax7T1"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_transformer = model._first_module().auto_model\n",
        "\n",
        "prefix_config = PrefixTuningConfig(\n",
        "    task_type=None,\n",
        "    num_virtual_tokens=20,\n",
        "    encoder_hidden_size=base_transformer.config.hidden_size,\n",
        ")\n",
        "\n",
        "peft_model = PeftModel(base_transformer, prefix_config)\n",
        "model._first_module().auto_model = peft_model"
      ],
      "metadata": {
        "id": "udtplYkCx8Tq"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Trainable parameters: {trainable_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0MorSAKyC3x",
        "outputId": "6c7282d7-7c83-4da0-85b2-ccba79002c4f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable parameters: 368640\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40V5cfgZyFem",
        "outputId": "02a97e98-386f-4551-8315-8a98bc51abff"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SentenceTransformer(\n",
              "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: PeftModel \n",
              "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationModel(nn.Module):\n",
        "    def __init__(self, base_model):\n",
        "        super(ClassificationModel, self).__init__()\n",
        "        self.base_model = base_model\n",
        "        self.classifier = nn.Linear(base_model.get_sentence_embedding_dimension(), 1)\n",
        "\n",
        "    def forward(self, tokenized_inputs):\n",
        "        embeddings = self.base_model(tokenized_inputs)[\"sentence_embedding\"]\n",
        "        logits = self.classifier(embeddings)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "iWYjVsD6yGv3"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classification_model = ClassificationModel(model).to(device)"
      ],
      "metadata": {
        "id": "psVjeAXeyIDC"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_params = filter(lambda p: p.requires_grad, classification_model.parameters())\n",
        "optimizer = AdamW(trainable_params, lr=0.0001)\n",
        "loss_fn = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "0fYf3aO_yJWf"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_per_epoch = []\n",
        "num_epochs = 20\n",
        "checkpoint_dir = \"model_checkpoints\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    classification_model.train()\n",
        "    all_train_predictions, all_train_labels = [], []\n",
        "    train_loss = 0.0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        tokenized_inputs, labels = batch\n",
        "        tokenized_inputs = {key: val.to(device) for key, val in tokenized_inputs.items()}\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = classification_model(tokenized_inputs)\n",
        "        loss = loss_fn(logits.squeeze(), labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        probabilities = torch.sigmoid(logits).squeeze()\n",
        "        predictions = (probabilities > 0.5).long()\n",
        "\n",
        "        all_train_predictions.extend(predictions.cpu().numpy())\n",
        "        all_train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    train_loss /= len(train_dataloader)\n",
        "    train_precision, train_recall, train_f1, train_support = precision_recall_fscore_support(\n",
        "        all_train_labels, all_train_predictions, average=None, zero_division = 0\n",
        "    )\n",
        "\n",
        "    # Validation phase\n",
        "    classification_model.eval()\n",
        "    all_val_predictions, all_val_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in validation_dataloader:\n",
        "            tokenized_inputs, labels = batch\n",
        "            tokenized_inputs = {key: val.to(device) for key, val in tokenized_inputs.items()}\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits = classification_model(tokenized_inputs)\n",
        "            loss = loss_fn(logits.squeeze(), labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            probabilities = torch.sigmoid(logits).squeeze()\n",
        "            predictions = (probabilities > 0.5).long()\n",
        "\n",
        "            all_val_predictions.extend(predictions.cpu().numpy())\n",
        "            all_val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    val_loss /= len(validation_dataloader)\n",
        "    val_precision, val_recall, val_f1, val_support = precision_recall_fscore_support(\n",
        "        all_val_labels, all_val_predictions, average=None, zero_division=0\n",
        "    )\n",
        "\n",
        "    # Save metrics for this epoch\n",
        "    epoch_metrics = {\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"train_loss\": train_loss,\n",
        "        \"val_loss\": val_loss,\n",
        "        \"train_precision\": train_precision,\n",
        "        \"train_recall\": train_recall,\n",
        "        \"train_f1\": train_f1,\n",
        "        \"val_precision\": val_precision,\n",
        "        \"val_recall\": val_recall,\n",
        "        \"val_f1\": val_f1\n",
        "    }\n",
        "    metrics_per_epoch.append(epoch_metrics)\n",
        "\n",
        "    base_model_path = os.path.join(checkpoint_dir, f\"epoch{epoch + 1}_gist_model.pth\")\n",
        "    torch.save(classification_model.base_model.state_dict(), base_model_path)\n",
        "\n",
        "    classifier_path = os.path.join(checkpoint_dir, f\"epoch{epoch + 1}_classifier_weights.pth\")\n",
        "    torch.save(classification_model.classifier.state_dict(), classifier_path)\n",
        "\n",
        "    model_path = os.path.join(checkpoint_dir, f\"epoch{epoch + 1}_classification_model.pth\")\n",
        "    torch.save(classification_model.state_dict(), model_path)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    print(f\"Training -> Loss: {train_loss}\")\n",
        "    print(f\"  Precision: {train_precision}    Recall: {train_recall}    F1: {train_f1}\")\n",
        "    print(f\"Validation -> Loss: {val_loss}\")\n",
        "    print(f\"  Precision: {val_precision}    Recall: {val_recall}    F1: {val_f1}\")\n",
        "    print(\"============================================================================================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVqLtfSHySGK",
        "outputId": "8c46bd9d-d437-4212-e462-b65593fc9c2b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "Training -> Loss: 0.5712124103020322\n",
            "  Precision: [0.7205448  0.70056972]    Recall: [0.7325659  0.68772348]    F1: [0.72650563 0.69408717]\n",
            "Validation -> Loss: 0.48074808616877934\n",
            "  Precision: [0.78719276 0.78571429]    Recall: [0.81187458 0.75862069]    F1: [0.79934319 0.77192982]\n",
            "============================================================================================================\n",
            "Epoch 2/20\n",
            "Training -> Loss: 0.46814678596875453\n",
            "  Precision: [0.79928405 0.7806671 ]    Recall: [0.80088422 0.7789493 ]    F1: [0.80008333 0.77980725]\n",
            "Validation -> Loss: 0.4310585240412025\n",
            "  Precision: [0.79848389 0.81626271]    Recall: [0.84322882 0.76595745]    F1: [0.82024659 0.79031037]\n",
            "============================================================================================================\n",
            "Epoch 3/20\n",
            "Training -> Loss: 0.4344083760207054\n",
            "  Precision: [0.81420307 0.79239606]    Recall: [0.81006006 0.79682773]    F1: [0.81212628 0.79460571]\n",
            "Validation -> Loss: 0.4095871570057043\n",
            "  Precision: [0.80805538 0.83110762]    Recall: [0.85657105 0.77622891]    F1: [0.83160622 0.80273141]\n",
            "============================================================================================================\n",
            "Epoch 4/20\n",
            "Training -> Loss: 0.416644605054962\n",
            "  Precision: [0.82348523 0.80564464]    Recall: [0.82307307 0.80608783]    F1: [0.8232791  0.80586618]\n",
            "Validation -> Loss: 0.3956843950062491\n",
            "  Precision: [0.82140554 0.82837529]    Recall: [0.84989993 0.79677183]    F1: [0.83540984 0.81226627]\n",
            "============================================================================================================\n",
            "Epoch 5/20\n",
            "Training -> Loss: 0.4069852895831662\n",
            "  Precision: [0.82988698 0.81050228]    Recall: [0.82691024 0.81369763]    F1: [0.82839594 0.81209681]\n",
            "Validation -> Loss: 0.38875446847364226\n",
            "  Precision: [0.81074329 0.8406027 ]    Recall: [0.86591061 0.77769626]    F1: [0.83741935 0.80792683]\n",
            "============================================================================================================\n",
            "Epoch 6/20\n",
            "Training -> Loss: 0.39828656268111484\n",
            "  Precision: [0.82941078 0.81215065]    Recall: [0.82899566 0.81259741]    F1: [0.82920317 0.81237397]\n",
            "Validation -> Loss: 0.379487656764478\n",
            "  Precision: [0.82718447 0.83219438]    Recall: [0.85256838 0.80410858]    F1: [0.83968463 0.81791045]\n",
            "============================================================================================================\n",
            "Epoch 7/20\n",
            "Training -> Loss: 0.3924577150267375\n",
            "  Precision: [0.83664404 0.81703758]    Recall: [0.832666   0.82130742]    F1: [0.83465028 0.81916693]\n",
            "Validation -> Loss: 0.37448959135809423\n",
            "  Precision: [0.82617062 0.838066  ]    Recall: [0.85923949 0.80117388]    F1: [0.84238064 0.8192048 ]\n",
            "============================================================================================================\n",
            "Epoch 8/20\n",
            "Training -> Loss: 0.390310994609097\n",
            "  Precision: [0.83627649 0.81432337]    Recall: [0.82957958 0.82149079]    F1: [0.83291457 0.81789137]\n",
            "Validation -> Loss: 0.37182261749019835\n",
            "  Precision: [0.82174463 0.84453125]    Recall: [0.86724483 0.79310345]    F1: [0.84388186 0.81800984]\n",
            "============================================================================================================\n",
            "Epoch 9/20\n",
            "Training -> Loss: 0.3838877012415585\n",
            "  Precision: [0.83901372 0.81674537]    Recall: [0.831665   0.82460805]    F1: [0.8353232  0.82065788]\n",
            "Validation -> Loss: 0.3684854790485105\n",
            "  Precision: [0.82375237 0.84753714]    Recall: [0.86991328 0.79530448]    F1: [0.84620376 0.82059046]\n",
            "============================================================================================================\n",
            "Epoch 10/20\n",
            "Training -> Loss: 0.38281359007565147\n",
            "  Precision: [0.8394576  0.82106321]    Recall: [0.83658659 0.82414963]    F1: [0.83801964 0.82260352]\n",
            "Validation -> Loss: 0.36514807596552973\n",
            "  Precision: [0.82804569 0.84914463]    Recall: [0.87058039 0.80117388]    F1: [0.84878049 0.82446206]\n",
            "============================================================================================================\n",
            "Epoch 11/20\n",
            "Training -> Loss: 0.37838984807728054\n",
            "  Precision: [0.84186595 0.81940285]    Recall: [0.83400067 0.827817  ]    F1: [0.83791485 0.82358843]\n",
            "Validation -> Loss: 0.36417206306031297\n",
            "  Precision: [0.82441787 0.85153181]    Recall: [0.87391594 0.79530448]    F1: [0.8484456  0.82245827]\n",
            "============================================================================================================\n",
            "Epoch 12/20\n",
            "Training -> Loss: 0.3765484621366948\n",
            "  Precision: [0.84164568 0.82121074]    Recall: [0.8361695  0.82708352]    F1: [0.83889865 0.82413667]\n",
            "Validation -> Loss: 0.3618000136407394\n",
            "  Precision: [0.83184143 0.84745763]    Recall: [0.86791194 0.80704329]    F1: [0.84949396 0.82675686]\n",
            "============================================================================================================\n",
            "Epoch 13/20\n",
            "Training -> Loss: 0.3762857531185004\n",
            "  Precision: [0.84167364 0.82366377]    Recall: [0.83900567 0.82653342]    F1: [0.84033754 0.8250961 ]\n",
            "Validation -> Loss: 0.3622413211028669\n",
            "  Precision: [0.82345602 0.85782367]    Recall: [0.88058706 0.79236977]    F1: [0.85106383 0.82379863]\n",
            "============================================================================================================\n",
            "Epoch 14/20\n",
            "Training -> Loss: 0.37430154321849307\n",
            "  Precision: [0.84561049 0.82441152]    Recall: [0.83883884 0.83166774]    F1: [0.84221106 0.82802373]\n",
            "Validation -> Loss: 0.358818984189846\n",
            "  Precision: [0.8290057  0.85190959]    Recall: [0.87324883 0.80190756]    F1: [0.85055231 0.82615268]\n",
            "============================================================================================================\n",
            "Epoch 15/20\n",
            "Training -> Loss: 0.37420844262209413\n",
            "  Precision: [0.84148695 0.8214611 ]    Recall: [0.83650317 0.82680847]    F1: [0.83898766 0.82412611]\n",
            "Validation -> Loss: 0.3557430788791379\n",
            "  Precision: [0.84052288 0.84009009]    Recall: [0.85790527 0.82098313]    F1: [0.84912512 0.83042672]\n",
            "============================================================================================================\n",
            "Epoch 16/20\n",
            "Training -> Loss: 0.3739875544208817\n",
            "  Precision: [0.84260811 0.82358843]    Recall: [0.83867201 0.827817  ]    F1: [0.84063545 0.8256973 ]\n",
            "Validation -> Loss: 0.3551246363607199\n",
            "  Precision: [0.83764554 0.8449848 ]    Recall: [0.86390927 0.8158474 ]    F1: [0.85057471 0.83016051]\n",
            "============================================================================================================\n",
            "Epoch 17/20\n",
            "Training -> Loss: 0.36976614080051873\n",
            "  Precision: [0.84431389 0.82440801]    Recall: [0.83917251 0.82992574]    F1: [0.84173535 0.82715767]\n",
            "Validation -> Loss: 0.3551959138176295\n",
            "  Precision: [0.83686577 0.84980843]    Recall: [0.86924616 0.81364637]    F1: [0.85274869 0.83133433]\n",
            "============================================================================================================\n",
            "Epoch 18/20\n",
            "Training -> Loss: 0.37117678873048304\n",
            "  Precision: [0.84551411 0.82503867]    Recall: [0.83958959 0.83139268]    F1: [0.84254144 0.82820349]\n",
            "Validation -> Loss: 0.35374905435399634\n",
            "  Precision: [0.83761232 0.85122699]    Recall: [0.87058039 0.81438004]    F1: [0.85377821 0.83239595]\n",
            "============================================================================================================\n",
            "Epoch 19/20\n",
            "Training -> Loss: 0.3688114122972465\n",
            "  Precision: [0.84273146 0.82390511]    Recall: [0.83900567 0.82790868]    F1: [0.84086444 0.82590204]\n",
            "Validation -> Loss: 0.3531458996361194\n",
            "  Precision: [0.83631714 0.85285054]    Recall: [0.87258172 0.81217902]    F1: [0.85406464 0.83201804]\n",
            "============================================================================================================\n",
            "Epoch 20/20\n",
            "Training -> Loss: 0.3687360251724595\n",
            "  Precision: [0.84519004 0.8239571 ]    Recall: [0.83842176 0.83120932]    F1: [0.84179229 0.82756732]\n",
            "Validation -> Loss: 0.35233430672624255\n",
            "  Precision: [0.83503185 0.85448916]    Recall: [0.87458306 0.80997799]    F1: [0.85434995 0.83163842]\n",
            "============================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_df = pd.DataFrame(metrics_per_epoch)\n",
        "metrics_df.to_csv(\"gte_headlines_prompt_epoch_metrics.csv\", index=False)"
      ],
      "metadata": {
        "id": "SpHkBk49yXTv"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GeCKiugC31v7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}